# Replicator Source Connector

Note: This utilizes the ability to manage Connector configurations, including Replicator, through a declarative Custom Resource. This ability was introduced in CFK 2.1.

In this example, you'll setup a Confluent Platform with Connect and install and manage the replicator source connector plugin through the declarative `Connector` CRD. 
Confluent Replicator is used to replicate data from one cluster to another, and so we'll need to two Kafka clusters - one Source and one Destination cluster. 
In this scenario, we deploy the Source cluster with SASL/Plain for authentication, and the Destination cluster with mTLS for authentication.
## Set up Pre-requisites

Set the tutorial directory for this tutorial under the directory you downloaded
the tutorial files:

```
export TUTORIAL_HOME=<Tutorial directory>/connector/replicator-source-connector
```

Create two namespaces, one for the source cluster components and one for the destination cluster components.
Note:: in this example, only deploy zookeeper and kafka for source and zookeeper, kafka and connect for destination

```
kubectl create ns source
kubectl create ns destination
```

Deploy Confluent for Kubernetes (CFK) in cluster mode, so that the one CFK instance can manage Confluent deployments in multiple namespaces. Here, CFk is deployed to the `default` namespace.

```
helm upgrade --install confluent-operator \
  confluentinc/confluent-for-kubernetes \
  --namespace default --set namespaced=false
```

Source components will be deployed in SASL_SSL mode
which requires a secret object for Kafka authentication and secrets for TLS

```
  kubectl -n source create secret generic credential \
  --from-file=plain-users.json=$TUTORIAL_HOME/creds-kafka-sasl-users.json \
  --from-file=plain.txt=$TUTORIAL_HOME/creds-client-kafka-sasl-user.txt 
  
  kubectl create secret generic tls-zookeeper \
    --from-file=fullchain.pem=$TUTORIAL_HOME/../../assets/certs/component-certs/generated/zookeeper-server.pem \
    --from-file=cacerts.pem=$TUTORIAL_HOME/../../assets/certs/component-certs/generated/cacerts.pem \
    --from-file=privkey.pem=$TUTORIAL_HOME/../../assets/certs/component-certs/generated/zookeeper-server-key.pem \
    --namespace source
  
  kubectl create secret generic tls-kafka \
    --from-file=fullchain.pem=$TUTORIAL_HOME/../../assets/certs/component-certs/generated/kafka-server.pem \
    --from-file=cacerts.pem=$TUTORIAL_HOME/../../assets/certs/component-certs/generated/cacerts.pem \
    --from-file=privkey.pem=$TUTORIAL_HOME/../../assets/certs/component-certs/generated/kafka-server-key.pem \
    --namespace source

```

Destination components will be deployed in mTLS mode, using AutoGenerated Certs. Connector will be created in destination namespace.
Secrets `src-kafka-credential` and `src-kafka-credential` is used by connector to communicate with source kafka cluster, which are mounted to Connect cluster.

```
  kubectl -n destination create secret generic src-kafka-credential \
  --from-file=plain-users.json=$TUTORIAL_HOME/creds-kafka-sasl-users.json \
  --from-file=plain.txt=$TUTORIAL_HOME/creds-client-kafka-sasl-user.txt 
  
  kubectl create secret generic src-tls-kafka \
    --from-file=fullchain.pem=$TUTORIAL_HOME/../../assets/certs/component-certs/generated/kafka-server.pem \
    --from-file=cacerts.pem=$TUTORIAL_HOME/../../assets/certs/component-certs/generated/cacerts.pem \
    --from-file=privkey.pem=$TUTORIAL_HOME/../../assets/certs/component-certs/generated/kafka-server-key.pem \
    --namespace destination
```


To use auto-generated certificates for Destination components. You'll need to generate and provide a Root Certificate Authority (CA).

Generate a CA pair to use in this tutorial:

```
openssl genrsa -out $TUTORIAL_HOME/ca-key.pem 2048
openssl req -new -key $TUTORIAL_HOME/ca-key.pem -x509 \
  -days 1000 \
  -out $TUTORIAL_HOME/ca.pem \
  -subj "/C=US/ST=CA/L=MountainView/O=Confluent/OU=Operator/CN=TestCA"
```

Then, provide the certificate authority as a Kubernetes secret `ca-pair-sslcerts` to be used to 
generate the auto-generated certs, in both the source and destination namespaces:

```
kubectl -n destination create secret tls ca-pair-sslcerts \
  --cert=$TUTORIAL_HOME/ca.pem \
  --key=$TUTORIAL_HOME/ca-key.pem 
```

## Deploy Source Components

```
kubectl apply -f $TUTORIAL_HOME/confluent-platform-source.yaml
```
Check that zookeeper, kafka cluster and topic `demo` are deployed:

```   
kubectl get confluent -n source
```

## Deploy Destination Components

```
kubectl apply -f $TUTORIAL_HOME/confluent-platform-destination.yaml
```
Check that zookeeper, kafka and connect cluster are deployed:

```   
kubectl get confluent -n destination
```

## Create Connector
Create connector 
```
kubectl apply -f $TUTORIAL_HOME/connector.yaml
```
Check connector 
```
kubectl get connector -n confluent
```

## Validation
Exec into source kafka pod
```
kubectl -n source exec kafka-0 -it -- bash
```

Create kafka.properties

```

cat <<EOF > /tmp/kafka.properties
bootstrap.servers=kafka.source.svc.cluster.local:9071
sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=kafka password=kafka-secret;
sasl.mechanism=PLAIN
security.protocol=SASL_SSL
ssl.truststore.location=/mnt/sslcerts/truststore.p12
ssl.truststore.password=mystorepassword
EOF
```

Produce in source kafka cluster

```
seq 10000 | kafka-console-producer --topic demo --broker-list kafka.source.svc.cluster.local:9071 --producer.config /tmp/kafka.properties
```

Open a new terminal and exec into destination kafka pod
```
kubectl -n destination exec kafka-0 -it -- bash
```
    
Create kafka.properties for destination kafka cluster
```
cat <<EOF > /tmp/kafka.properties
bootstrap.servers=kafka.destination.svc.cluster.local:9071
security.protocol=SSL
ssl.truststore.location=/mnt/sslcerts/truststore.jks
ssl.truststore.password=mystorepassword
ssl.keystore.location=/mnt/sslcerts/keystore.jks
ssl.keystore.password=mystorepassword
EOF
```
    
Validate topic is created in destination kafka cluster
```
kafka-topics --describe --topic demo --bootstrap-server kafka.destination.svc.cluster.local:9071 --command-config /tmp/kafka.properties
```

Consume in destination kafka cluster and confirm message delivery in destination cluster

```
kafka-console-consumer --from-beginning --topic demo --bootstrap-server  kafka.destination.svc.cluster.local:9071  --consumer.config /tmp/kafka.properties
```


## Tear down

```
kubectl delete -f $TUTORIAL_HOME/connector.yaml
kubectl delete -f $TUTORIAL_HOME/confluent-platform-destination.yaml
kubectl delete -f $TUTORIAL_HOME/confluent-platform-source.yaml
```
